<!DOCTYPE HTML>
<!--
	Stellar by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Neuro-Symbolic Computation and Machine Common Sense</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="assets/css/main.css?v=0.0" />
		<!--[if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif]-->
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
	</head>
	<body>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header" class="alt">
						<!-- <span class="logo"><img src="images/logo_mit_ibm.png" alt="" /></span> -->
						<h1>AI Week 2019: Neuro-Symbolic Computation and Machine Common Sense</h1>
						<p> Location: MIT Samberg Center<br /> (<a href="#dir">Getting Here</a>)</p>
						<p> Wednesday, September 18, 2019 </p>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul>
							<li><a href="#intro" class="active">Introduction</a></li>
							<!--<li>Registration</li>-->
							<!--<li><a href="#cfp">Call For Participation</a></li>-->
							<li><a href="#sched">Schedule</a></li>
							<li><a href="#reg">Registration</a></li>
							<li><a href="#inv">Invited Speakers</a></li>
							<!-- <li><a href="#panel">Featured Panelists</a></li> -->
							<li><a href="#org">Organizers</a></li>
							<li><a href="https://ibm.biz/ai-research-week">IBM Research AI Week</a></li>
							<!-- <li><a href="https://researcher.watson.ibm.com/researcher/view_group_subpage.php?id=7848">Previous Summits</a></li> -->
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">
						<!-- Introduction -->
							<section id="intro" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>About</h2>
										</header>
										<!-- <p style="text-align:center"><img src="images/banner_1.jpg" alt="MIT Watson AI Lab, Cambridge, MA"
											width="1024" height="256">
										</p> -->
										<p style="text-align:justify"> On Wednesday, September 18th, 2019 as part of <a href="https://ibm.biz/ai-research-week">IBM Research's
											AI week</a> we will be hosting the first workshop on <b>Neuro-Symbolic Computation and Machine Common Sense</b> (NSC-MCS).
										 </p>

										<p  style="text-align:justify"> Neuro-symbolic methods are relatively new in the DL community and at present, they are largely driven by the findings at the intersection of the fields of child-psychology, generative modelling and neuroscience suggesting symbol manipulation may be at the core of human common sense. This workshop will bring researchers from the aforementioned fields together with the highly diverse set of researchers in the wider fields of representation learning and reasoning in order to advance discussion and promote collaborations across its various subdomains, with a special focus on Neuro-symbolic AI and Neuro-Symbolic Computing and Machine Common Sense.
										</p>




									</div>

							</section>

						<!-- First Section 
							<section id="cfp" class="main special">
								<header class="major">
									<h2>Call For Participation</h2>
								</header>
								<p style="text-align:justify">
						 We invite researchers to submit work in (but not limited to) the following areas:
		<ul style="text-align:left">
		<li> Neuro-symbolic models </li>
		<li> TBD </li>
	</ul>
</P>

<h2 style="text-align:left">Submissions</h2>
<p style="text-align:justify">
	Submission can be made via an
	<a href=https://easychair.org/conferences/>
		EasyChair submission.</a>
		The submission should be in the form of an extended abstract and should not exceed 3 pages (excluding references) in PDF format using NeurIPS style. Submissions of new ideas, recently published works and/or extension of existing works are welcome. Parallel submissions or submissions of under-review works are also permitted. Author names do not need to be anonymized.

		Submission will be accepted as contributed 15-minute talks or poster presentations. The final versions will be posted on the workshop website (and are archival but do not constitute a proceeding). 
	</p>

	<h2 style="text-align:left"> Key Dates </h2>
	<ul style="text-align:left">
		<li>Abstracts due: Abstract submission is now closed. </li>
		<li>Notification to Submitters: 03/26/2018. </li>
		<li> Meeting Date: 04/27/2018 </li>
	</ul>


	<h2 style="text-align:left">Attendance</h2>
	<p style="text-align:justify"> For each accepted paper or poster,
	at least one author must attend the workshop and present the
	paper/poster.
	<br>
-->

	</section>

						<!-- Agenda -->
							<section id="sched" class="main special">
								<header class="major">
									<h2>Schedule (To be Finalized!)</h2>
								</header>
								<table style="width:100%">
								<tr>
									<td>9:00-9:15 AM</td>
									<td>Welcome and Opening Remarks </br> TBD</td>
								</tr>
								<tr>
									<td>9:15-12:00 AM</td>
									<td> 4 invited talks </br> (coffee break in the middle) </br> </td>
								</tr>
								 <tr>
									<td>12:00-1:00 PM</td>
									<td>Lunch</td>
								</tr>
								<tr>
									<td>1:00-1:45 PM	</td>
									<td>TDW Tutorial</br></td>
								</tr>
								<tr>
									<td>1:45-2:30 PM	</td>
									<td>TDW Use Cases</br></td>
								</tr>
								<tr>
									<td>2:30-2:45 PM</td>
									<td> Coffee Break </td>
								</tr>
								<tr>
									<td>2:45-5:00 PM</td>
									<td>4 invited talks</td>
								</tr>
							</table>
							</section>

							<!-- Invited Speakers -->
							<section id="inv" class="main special">
								<header class="major">
									<h2>Invited Speakers</h2>
								</header>
								<table style="float:center">

									<tr>
										<td>
											<div class="image" style="float:left;width: 30%" align="center">
												<img src="images/speakers/daniel_yamins.jpg" height="192"/>
												<div><h2> <a href="http://stanford.edu/~yamins/">Daniel Yamins</a> </h2></div>
											</div>
											Daniel Yamins is a cognitive computational neuroscientist at Stanford University, where he's an assistant professor of Psychology and Computer Science,
											a faculty scholar at the Wu Tsai Neurosciences Institute, and an affiliate of the Stanford Artificial Intelligence Laboratory. 
											His research group focuses on reverse engineering the algorithms of human cognition, both to learn both about how our brains work and build more effective
											artificial intelligence systems. He is especially interested in how brain circuits for sensory information processing and decision making arise via
											the optimization of high-performing cortical algorithms for key behavioral tasks. He received his AB and PhD degrees from Harvard University,
											was a postdoctoral research at MIT, and has been a visiting researcher at Princeton University and Los Alamos National Laboratory.
											He is a recipient of an NSF Career Award, the James S. McDonnell Foundation award in Understanding Human Cognition, the Sloan Research Fellowship,
											and is a Simons Foundation Investigator. 
										</td>
									</tr>
									

									<tr>
										<td>
											<div class="image" style="float:left;width: 30%" align="center">
												<img src="images/speakers/rebecca_saxe3.jpg" height="192"/>
												<div><h2> <a href="https://saxelab.mit.edu/">Rebecca Saxe</a> </h2></div>
											</div>
											Rebecca Saxe is an associate investigator of the McGovern Institute and the John W. Jarve (1978) Professor in Brain and Cognitive Sciences at MIT. 
											She obtained her Ph.D. from MIT and was a Harvard Junior Fellow before joining the MIT faculty in 2006. She was awarded tenure in 2011.
											Saxe was chosen In 2012 as a Young Global Leader by the World Economic Forum, and she received the 2014 Troland Award from the National Academy of Sciences.
											Rebecca Saxe studies human social cognition, using a combination of behavioral testing and brain imaging technologies. 												
										</td>
									</tr>
	
									<tr>
										<td>
											<div class="image" style="float:left;width: 30%" align="center">
												<img src="images/speakers/kate_saenko.jpg" height="192"/>
												<div><h2> <a href="http://ai.bu.edu/ksaenko.html">Kate Saenko</a> </h2></div>
											</div>
											Kate Saenko is an Associate Professor of Computer Science at Boston University and a consulting professor for the MIT-IBM Watson AI Lab.
											She leads the Computer Vision and Learning Group at BU, is the founder and co-director of the Artificial Intelligence Research (AIR) initiative,
											and member of the Image and Video Computing research group. Kate received a PhD from MIT and did her postdoctoral training at UC Berkeley and Harvard.
											Her research interests are in the broad area of Artificial Intelligence with a focus on dataset bias, adaptive machine learning, learning for image and
											language understanding, and deep learning.
										</td>
									</tr>
	
									<tr>
										<td>
											<div class="image" style="float:left;width: 30%" align="center">
												<img src="images/speakers/josh_tenenbaum.jpg" height="192"/>
												<div><h2> <a href="https://web.mit.edu/cocosci/josh.html">Josh Tenenbaum</a> </h2></div>
											</div>						
											Josh Tenenbaum is a professor of Computational Cognitive Science in the Department of Brain and Cognitive Sciences at MIT, and a member of the Computer Science and Artificial Intelligence Laboratory (CSAIL).
											He studies learning, reasoning and perception in humans and machines, with the twin goals of understanding human intelligence in computational terms and bringing computers closer to human capacities.
									</td>
									</tr>
									<tr>
										<td>
											<div class="image" style="float:left;width: 30%" align="center">
												<img src="images/organizers/jayram.jpg" height="192"/>
												<div><h2><a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-jayram">T.S. Jayram</a> </h2></div>
											</div>						
											T.S. Jayram (Jayram Thathachar) is a research scientist in the Machine Intelligence Team at IBM Research AI. His work in deep learning focuses on cognitively inspired memory-augmented neural network architectures with
											 a view towards visually grounded language learning and reasoning with memory. This was inspired partly by his past work on the role of memory in theoretical computer science where he has made fundamental
											 contributions to both algorithms in data science as well as demonstrating their limitations using information theory. For these contributions, which includes starting two new areas namely, Information
											 Complexity in theoretical computer science and Index Coding in information theory, he was awarded the IBM Research Accomplishment (Outstanding) in the Science category (along with David Woodruff) in 2012.
											 He was also an invited speaker at the ACM PODS conference in 2010. In the past he has worked on time-space tradeoffs for branching programs; part of this work won the Machtey Award in FOCS 1994.						
										</td>
									</tr>

									
									<tr>
										<td>
											<div class="image" style="float:left;width: 30%" align="center">
												<img src="images/speakers/akash_srivastava.jpg" height="192"/>
												<div><h2> <a href="https://akashgit.github.io/">Akash Srivastava</a> </h2></div>
											</div>
											Akash Srivastava a research scientist at the MIT-IBM lab in Cambridge, MA where he works on building machines with child-like common-sense and
											intuitive physics using probabilistic modelling and Bayesian inference. Before this, he was a PhD student in the Informatics Forum at
											University of Edinburgh where he worked with Dr Charles Sutton and Dr Michael U. Gutmann on variational inference for generative models using deep learning.
										</td>
									</tr>

									<tr>
										<td>
											<div class="image" style="float:left;width: 30%" align="center">
												<img src="images/speakers/tomer_ullman.jpg" height="192"/>
												<div><h2> <a href="http://www.mit.edu/~tomeru/">Tomer D. Ullman</a> </h2></div>
											</div>
											Tomer D. Ullman is a postdoctoral associate in the Computational Cognitive Science group at MIT, and the Lab for Developmental Studies at Harvard.
											He studies people's common-sense reasoning about physics and psychology, using computational models and behavioral experiments with children and adults.
											His research is funded by the Center for Brains, Minds and Machines. 
										</td>
									</tr>


									

									<tr>
										<td>
											<div class="image" style="float:left;width: 30%" align="center">
												<img src="images/speakers/jiajun_wu.jpg" height="192"/>
												<div><h2> <a href="https://jiajunwu.com">Jiajun Wu</a> </h2></div>
											</div>
											Jiajun Wu is a Ph.D. student in Electrical Engineering and Computer Science at Massachusetts Institute of Technology.
											He received his B.Eng. from Tsinghua University in 2014. His research interests lie in the intersection of computer vision, machine learning, robotics,
											and computational cognitive science.
											His research has been recognized through the IROS Best Paper Award on Cognitive Robotics and fellowships from Facebook, Nvidia, Samsung, Baidu, and Adobe,
											and his work has been covered by major media outlets including CNN, BBC, WIRED, and MIT Tech Review.
										</td>
									</tr>
	

								</table>

							</section>

							<section id="org" class="main special">
								<header class="major">
									<h2>ThreeDWorld Tutorial</h2>
									<h3>Researching Intelligence using Virtual Worlds</h3>
								</header>								
								ThreeDWorld (TDW) is a simulation framework that uses state-of-the-art video game technology to generate photo-realistic scenes in a virtual world. TDW’s flexible, general design allows researchers to collect next-generation experimental data and generate large-scale datasets for training AI systems with complete control over data generation and full access to all associated generative parameters.

								<br><br>
								<h2>Tutors</h2>
								<table style="float:center">
									<tr>
										<td>
											<div class="image" style="float:left;width: 20%" align="center">
												<img src="images/tdw/seth_alter.jpg" height="128"/>
												<div>Seth Alter</div>
											</div>																				
											Seth Alter is the back-end programmer and API developer for TDW. He is an experienced game designer and developer, specializing in python, Unity 3D development and procedural content creation. He is also the co-organizer of Boston Indies (a local monthly meetup group for independent game developers).										</td>
									</tr>

									<tr>
										<td>
											<div class="image" style="float:left;width: 20%" align="center">
												<img src="images/tdw/jeremy_schwartz2.jpg" height="128"/>
												<div>Jeremy Schwartz</div>
											</div>																				
											Jeremy Schwartz is the Project Lead for TDW. He is a veteran interactive technology developer and CGI production manager, 
											who has worked for many years in the video game industry. While at Electronic Arts and Acclaim Entertainment he ran studio 
											operations for motion capture and audio/video post-production, and managed 3D animation and tool-development teams. Jeremy 
											also spent several years as a technology analyst with Forrester Research, and has worked for design consultancies and 
											interactive agencies developing both digital and hybrid physical/digital interactive experiences.
										</td>
									</tr>
								</table>								
							</section>

							<section id="org" class="main special">
								<header class="major">
									<h2>ThreeDWorld Use Cases</h2>

								</header>								
								 								
								<table style="float:center">
									<tr>
										<td>
											<h3>Use Case I</h3>
											The aim of our project is to understand the underlying neural computations involved in intuitive physics, i.e., common sense
											about physical interactions. We present video stimuli of physical events unfolding over time to rhesus macaques and compare
											the brain response (with fMRI and fMRI-guided electrophysiology) to intentional interactions versus purely physical interactions.
											Crucially, by using TDW to create parametric animations we will achieve unprecedented control over relevant physical properties
											(like magnitude and direction of force), which we can subsequently decode from neural activity using computational models.
											<br> <br><h3>Speaker</h3>
											<div class="image" style="float:left;width: 20%" align="center">
												<img src="images/tdw/ronald_alvarez2.jpg" height="128"/>
												<div>Ronald Alvarez</div>
											</div>						
											Ronald  is an Electrical Engineering and Mathematics student at Mercer University. He works on developing computational
											models of planning and decision-making with the Tenenbaum lab at MIT, investigating neural correlates of intuitive physics
											with the Freiwald lab at Rockefeller University, and control theory with the Thitsa lab at Mercer.
										</td>
									</tr>
									<tr>
										<td>
											<h3>Use Case II</h3>
											Computer Vision researchers have limited control over various dimensions such as object size, angle, etc. in
											large-scale photorealistic datasets. Using TDW, we can generate extremely flexible and fully controlled datasets
											that allow for training models with transfer performances approaching those of photographic datasets such as ImageNet.									
											<br> <br><h3>Speaker</h3>
											<div class="image" style="float:left;width: 20%" align="center">
												<img src="images/tdw/martin_schrimpf.jpg" height="128"/>
												<div>Martin Schrimpf</div>
											</div>						
											Martin completed his Bachelor and Master degrees in Software Engineering in Germany at TUM, LMU and UNA, 
											conducting his Master’s Thesis on recurrent visual processing at Harvard with Gabriel Kreiman. 
											He has founded the non-profit startup Integreat and worked at Salesforce Research on deep learning for NLP 
											with Richard Socher before joining MIT’s Brain and Cognitive Sciences program for his PhD. There, he is 
											advised by James DiCarlo and works on evaluating and building neural network models of brain processing.
										</td>
									</tr>
									<tr>
										<td>
											<h3>Use Case II</h3>
											Humans can readily interpret the sounds made when objects collide,
											scrape, and clatter against each other. We know little about how humans
											can make such inferences, nor can we replicate their success with
											machine hearing algorithms. We are building a perceptually inspired
											generative model of contact sounds, which can be interfaced with TDW to
											simulate sounds made by everyday objects.  We can use these sounds to
											study human perception of natural sounds.
											<br> <br><h3>Speaker</h3>
											<div class="image" style="float:left;width: 20%" align="center">
													<img src="images/tdw/james_traer2.jpg" height="128"/> 
												<div>James Traer</div>
											</div>						
											James was formally trained as a physicist, and during his PhD work
											(Univ. of California, San Diego) he studied sound generation by ocean
											waves and developed algorithms to map the ocean and the Earth's crust
											with ambient sound. In 2013 he joined the
											McDermott Laboratory for Computational Audition (MIT) as a
											post-doctoral researcher. He has since studied how humans infer physical
											attributes of the world (e.g. source distance, room size, object
											material, force of impact) from the sounds made by everyday objects as
											they collide, scrape and deform.
										</td>
									</tr>

								</table>
	
							</section>

							<!-- Registration -->
							<section id="reg" class="main special">
								<header class="major">
									<h2>Registration</h2>
								</header>
								
								<h3>Registration to the Neuro-Symbolic Computing and Machine Common Sense workshop is open!</h3>
								
								<a href="https://neuro-symbolic-computing-machine-common-sense.eventbrite.com"><img src="images/eventbrite_image.jpg"></a>
								
								<br> 
								In order to register please use <a href="https://neuro-symbolic-computing-machine-common-sense.eventbrite.com">Eventbride</a> website.
								<br>
								<br>
								<br>

							<!-- Organizers -->
							<section id="org" class="main special">
								<header class="major">
									<h2>Organizing Committee</h2>
								</header>
		

								<!-- @Akash, @tklinger, @Masataro, @christian.muise, @asozcan, and @jayram -->
								<table style="float:center">
									<tr>
										<td>
												<div class="image" style="float:left;width: 20%" align="center">
														<img src="images/organizers/masataro_asai.jpg" height="128"/>
													<div><a href="https://researcher.watson.ibm.com/researcher/view.php?person=ibm-Masataro.Asai">Masataro Asai</a></div>
													<div>MIT-IBM Watson AI Lab, Cambridge, MA, USA</div>
												</div>						
												<div class="image" style="float:left;width: 20%" align="center">
														<img src="images/organizers/abhishek_bhandwaldar.jpg" height="128"/>
													<div><a href="https://w3.ibm.com/bluepages/profile.html?uid=2J3294897">Abhishek Bhandwaldar</a></div>
													<div>MIT-IBM Watson AI Lab, Cambridge, MA, USA</div>
												</div>						
												<div class="image" style="float:left;width: 20%" align="center">
														<img src="images/organizers/dan_gutfreund.jpg" height="128"/>
													<div><a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-dgutfre">Dan Gutfreund</a></div>
													<div>MIT-IBM Watson AI Lab, Cambridge, MA, USA</div>
												</div>						
												<div class="image" style="float:left;width: 20%" align="center">
														<img src="images/organizers/jayram.jpg" height="128"/>
													<div><a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-jayram">T.S. Jayram</a></div>
													<div>Almaden Research Center, San Jose, CA, USA</div>
												</div>						
												<div class="image" style="float:left;width: 20%" align="center">
														<img src="images/organizers/tim_klinger.jpg" height="128"/>
													<div><a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-tklinger">Tim Klinger</a></div>
													<div>Thomas J. Watson Research Center, Yorktown Heights, NY, USA</div>
												</div>						
	
										</td>
									</tr>
									<tr>
										<td> 
											<div class="image" style="float:left;width: 25%" align="center">
													<img src="images/organizers/tomasz_kornuta.jpg" height="128"/>
												<div><a href="tomaszkornuta.com">Tomasz Kornuta</a></div>
												<div>Almaden Research Center, San Jose, CA, USA</div>
											</div>						
											<div class="image" style="float:left;width: 25%" align="center">
													<img src="images/organizers/christian_muise.jpg" height="128"/>
												<div><a href="https://researcher.watson.ibm.com/researcher/view.php?person=ibm-Christian.Muise">Christian Muise</a></div>
												<div>MIT-IBM Watson AI Lab, Cambridge, MA, USA</div>
											</div>						
											<div class="image" style="float:left;width: 25%" align="center">
													<img src="images/organizers/ahmet_ozcan.jpg" height="128"/>
												<div><a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-asozcan">Ahmet S. Ozcan</a></div>
												<div>Almaden Research Center, San Jose, CA, USA</div>
											</div>						
											<div class="image" style="float:left;width: 25%" align="center">
													<img src="images/organizers/akash_srivastava.jpg" height="128"/>
												<div><a href="https://akashgit.github.io/">Akash Srivastava</a></div>
												<div>MIT-IBM Watson AI Lab, Cambridge, MA, USA</div>
											</div>						

										</td>
									</tr>
								</table>

							</section>

							<!-- Spotlights 
							<section id="spot" class="main special">
								<header class="major">
									<h2>Posters</h2>
								</header>
								<table style="width:100%">
							</table>
							</section>
-->
					</div>

				<!-- Footer -->
					<footer id="footer">
						<section id="dir">
							<h2>Location</h2>
							<p style="text-align:justify">75 Binney St, Cambridge, MA 02142</br>
							<i>Samberg center is located in Kendall square and easily accessible by public transportaion.
						  It is a short walk from the <a href=https://www.google.com/maps/dir/Kendall,+Cambridge,+MA+02142/MIT+Samberg+Conference+Center,+Memorial+Drive,+Cambridge,+MA/@42.3614109,-71.0867415,17z/data=!3m1!4b1!4m14!4m13!1m5!1m1!1s0x89e370af454cadd1:0xed3f24407286838c!2m2!1d-71.0855752!2d42.3624823!1m5!1m1!1s0x89e370a679984489:0x5c8d65db5c0d7efe!2m2!1d-71.0836961!2d42.360732!3e2>Kendall/MIT</a> stop on the red line and from <a href="https://www.google.com/maps/dir/Lechmere,+Cambridge,+MA/MIT+Samberg+Conference+Center,+Memorial+Drive,+Cambridge,+MA/@42.3652177,-71.0851221,16z/data=!3m1!4b1!4m14!4m13!1m5!1m1!1s0x89e370be837d0fa7:0x1c5777c77406d5f9!2m2!1d-71.077113!2d42.370088!1m5!1m1!1s0x89e370a679984489:0x5c8d65db5c0d7efe!2m2!1d-71.0836961!2d42.360732!3e2">Lechmere</a> on the green line.
							We highly encourage using public transportation to get here. </i></p>


						</section>
						<section>
							<h2>Contact</h2>
								<dd>Dan Gutfreund, dgutfre(ta)us(tod)ibm(tod)com</dd>
								<dd>Christian Muise, christian(tod)muise(ta)ibm(tod)com</dd>
								<dd>Akash Srivastava, akash(tod)srivastava(ta)ibm(tod)com</dd>
						</section>
						<p class="copyright">&copy; Workshop Organizing Committee. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
					</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="assets/js/main.js"></script>

	</body>
</html>
